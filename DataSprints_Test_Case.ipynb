{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a680984",
   "metadata": {},
   "source": [
    "# Technical Test Case DataSprints\n",
    "\n",
    "This notebook has the objective to complete the tasks given for the techincal data science case for recruiting, using Python and SQL skills to complete the tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c3cc29",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "-> [Environment Preparation](#Installs)\n",
    "\n",
    "-> [Loading and Exploring Data](#Loading-and-Exploring-Data)\n",
    "\n",
    "* [Trips Data](#Trips-Data)\n",
    "\n",
    "* [Payment Data](#Payment-Lookup-Data)\n",
    "\n",
    "* [Vendors Data](#Vendors-Data)\n",
    "\n",
    "-> [Analysis and Tasks](#Analysis-and-Tasks)\n",
    "\n",
    "* [Question 1](#Question-1)\n",
    "\n",
    "* [Question 2](#Question-2)\n",
    "\n",
    "* [Question 3](#Question-3)\n",
    "\n",
    "* [Question 4](#Question-4)\n",
    "\n",
    "-> [Bonus Tasks](#Bonus-Tasks)\n",
    "\n",
    "* [Bonus 1](#Bonus-1)\n",
    "\n",
    "* [Bonus 2](#Bonus-2)\n",
    "\n",
    "* [Bonus 3](#Bonus-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e62be3",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ae778",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.3.4\n",
    "!pip install numpy==1.20\n",
    "!pip install plotly==5.3.1\n",
    "!pip install datashader==0.13.0\n",
    "!pip install dash==2.0.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68469a",
   "metadata": {},
   "source": [
    "## Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8172a06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8110cd",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc188e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dateutil.parser\n",
    "from datetime import datetime\n",
    "from colorcet import fire, kbc\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import json\n",
    "import dash\n",
    "from dash.dependencies import Output, Input\n",
    "import dash_core_components as dcc\n",
    "from dash import html\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a307da",
   "metadata": {},
   "source": [
    "# Loading and Exploring Data\n",
    "\n",
    "We will create a local database with the data we have been given from the **NYC Taxi Trips** database, first loading the data from JSON and CSV files, transforming and appending where it's necesssary, then passing it all into tables in the local database. We will also be checking for data inconsistincy and missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712489e1",
   "metadata": {},
   "source": [
    "Checking the data files in our directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d1ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1358a3",
   "metadata": {},
   "source": [
    "## Trips Data\n",
    "\n",
    "Here the data is stored in seperate JSON files, apparently by year. We will read the data and make it into a single table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845b72ca",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df443e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Creating empty dataframe to populate with the Trips data\n",
    "trips_df = pd.DataFrame()\n",
    "\n",
    "#Iterating through each year of the data files\n",
    "for year in range(2009,2013):\n",
    "    \n",
    "    #Reading json file from the iterating year into a temporary dataframe\n",
    "    df = pd.read_json(f'data-sample_data-nyctaxi-trips-{year}-json_corrigido.json', lines=True)\n",
    "    \n",
    "    #If trips_df is still empty, passes the temporary dataframe to the trips_df\n",
    "    if trips_df.empty:\n",
    "        trips_df = df\n",
    "    \n",
    "    #Else, appends the temporary dataframe to the end of the trips_df\n",
    "    else:\n",
    "        trips_df = trips_df.append(df)\n",
    "    \n",
    "\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19c56f3",
   "metadata": {},
   "source": [
    "### Checking data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0f888e",
   "metadata": {},
   "source": [
    "Checking NAN values in the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad06cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('__NAN Values__')\n",
    "\n",
    "#Iterating through each column\n",
    "for column in trips_df.columns:\n",
    "    \n",
    "    #Sums all rows that are True for NAN value\n",
    "    nans = trips_df[f'{column}'].isna().sum()\n",
    "    \n",
    "    print(f'{column}: {nans}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe5d075",
   "metadata": {},
   "source": [
    "Checking column data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72887f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4a554",
   "metadata": {},
   "source": [
    "Dataframe size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23bd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26119da1",
   "metadata": {},
   "source": [
    "Dataframe memory usage in megabytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353406c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_mb = trips_df.memory_usage(index=True).sum() * 0.000001\n",
    "memory_mb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680082ae",
   "metadata": {},
   "source": [
    "General view of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed8481",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b99295a",
   "metadata": {},
   "source": [
    "### Improving Data format\n",
    "\n",
    "Datetime is in ISO format, which causes compatibility problems with SQLite, and is also not very good for visualising a dataframe, so we will change the datetime into a better format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cdbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing the ISO datetime format and getting the datetime object\n",
    "datetimes = [dateutil.parser.isoparse(c) for c in trips_df['pickup_datetime']]\n",
    "\n",
    "#Changing datetime format to a more conventional one that SQLite can read\n",
    "datetimes = [c.strftime('%Y-%m-%d %H:%M:%S') for c in datetimes]\n",
    "trips_df['pickup_datetime'] = datetimes\n",
    "\n",
    "#Parsing the ISO datetime format and getting the datetime object\n",
    "datetimes = [dateutil.parser.isoparse(c) for c in trips_df['dropoff_datetime']]\n",
    "\n",
    "#Changing datetime format to a more conventional one that SQLite can read\n",
    "datetimes = [c.strftime('%Y-%m-%d %H:%M:%S') for c in datetimes]\n",
    "trips_df['dropoff_datetime'] = datetimes\n",
    "\n",
    "\n",
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a619faa",
   "metadata": {},
   "source": [
    "### Removing columns\n",
    "\n",
    "The _rate_code_ column is just a waste of space since its entirely populated by NANs, so we will drop it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe4358b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_df = trips_df.drop('rate_code',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b79764",
   "metadata": {},
   "source": [
    "## Payment Lookup Data\n",
    "\n",
    "Here the data is stored in a single CSV file which helps loading in data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f198cb",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf2ec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df = pd.read_csv('data-payment_lookup-csv.csv', header=1)\n",
    "payment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ccccd6",
   "metadata": {},
   "source": [
    "### Checking data\n",
    "\n",
    "This data looks wrong, let's check the size of the dataset, then how many lines have these Foo values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d9876",
   "metadata": {},
   "source": [
    "Looking at the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c5ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae9f032",
   "metadata": {},
   "source": [
    "Looking at how many rows have the value _Foo_ in the _payment_lookup_ column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0d5a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df[payment_df['payment_lookup']=='Foo'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c119146",
   "metadata": {},
   "source": [
    "Checking the different values we have in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a8c62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df['payment_lookup'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b780fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "payment_df['payment_type'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c8fced",
   "metadata": {},
   "source": [
    "The _payment_type_ column has strange values considering it's relation with the _payment_lookup_ column.\n",
    "\n",
    "Let's check the cardinality of the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca067c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(payment_df['payment_type'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51dcf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(payment_df['payment_lookup'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2d458e",
   "metadata": {},
   "source": [
    "There are very few lines with usefull information in the _payment_type_ column, all the rest have these Foo values. The _payment_type_ column also has strange values and has a very **high cardinality** .Looks like this dataset has the wrong values in it. We won't need this dataset anyways, since payment information is already included in the Trips dataset, so we will move to the next data file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0112e5d",
   "metadata": {},
   "source": [
    "## Vendors Data\n",
    "\n",
    "Here the data is also stored in a single CSV file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316ec98d",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d0f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_df = pd.read_csv('data-vendor_lookup-csv.csv')\n",
    "vendor_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbe8337",
   "metadata": {},
   "source": [
    "### Checking data\n",
    "\n",
    "This dataframe luckly has only 5 rows so we can check it just by looking at it, data looks ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6bea15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vendor_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da21131",
   "metadata": {},
   "source": [
    "## Creating Local Database\n",
    "\n",
    "Now we will gather the datasets we need into a database with 1 table, the **Trips table**. In a way that we can use SQL queries on the database later on in the analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28505b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Creates new local database if one doesn't already exist\n",
    "conn = sqlite3.connect(\"Trips_and_vendors.db\")\n",
    "\n",
    "\n",
    "#Storing both dataframes in the database as tables, removing tables if they already exist, for pushing changes.\n",
    "\n",
    "try: trips_df.to_sql('Trips', conn)\n",
    "except Exception as e:\n",
    "    \n",
    "    if \"Table 'Trips' already exists.\" in str(e):\n",
    "        conn.cursor().execute('DROP TABLE Trips')\n",
    "        trips_df.to_sql('Trips', conn)\n",
    "        \n",
    "    else: raise e\n",
    "\n",
    "\n",
    "\n",
    "#Query to test if we can perform queries on the database\n",
    "query = 'SELECT * FROM Trips'\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "temp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091764c4",
   "metadata": {},
   "source": [
    "# Analysis and Tasks\n",
    "\n",
    "Now that we have our local database, we will start analysing the data and answer the questions given for the technical test case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a7f091",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "_**What is the average distance traveled by trips with a maximum of 2 passengers?**_\n",
    "\n",
    "So we need to get the average distance traveled on trips that have **not more than 2 passengers**, very important detail on maximum of 2 passengers, since the _passenger_count_ column has 0 values. So we need to include these data entries too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24984e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get trip distance from trips with a maximum of 2 passengers\n",
    "query = '''SELECT trip_distance \n",
    "FROM Trips\n",
    "WHERE passenger_count <= 2\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "#Query to get the average of the trip distance, just to show knowledge of aggregating functions\n",
    "query = '''SELECT AVG(trip_distance ) as avg_distance\n",
    "FROM Trips\n",
    "WHERE passenger_count <= 2\n",
    "'''\n",
    "\n",
    "#Reading the query result\n",
    "avg = pd.read_sql(query, conn)['avg_distance'].values[0]\n",
    "\n",
    "fig = px.histogram(temp_df, x='trip_distance', y='trip_distance', title='Max 2 Passengers Trip Distance Distribution', nbins=70)\n",
    "fig.add_vline(x=avg, line_dash = 'dash', line_color = 'firebrick',annotation_text=f\"Mean: {avg}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5167f46",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "_**Which are the 3 biggest vendors based on the total amount of money raised?**_\n",
    "\n",
    "So we need to sum the total ammount of money raised by each vendor and compare them. Total amount of money collected per trip can be found in the _total_amount_ column. This could be a trick question, since tip money probably doesn't go to the vendors, but for the purpose of getting straight to the point we won't dive into the matter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get the total amount of money raised per vendor, then limiting it to the top 3 vendors\n",
    "query = '''SELECT vendor_id, SUM(total_amount) as total_money_raised\n",
    "FROM Trips\n",
    "GROUP BY vendor_id\n",
    "ORDER BY total_money_raised desc\n",
    "LIMIT 3\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "fig = px.bar(temp_df, x='vendor_id', y='total_money_raised', title='3 Biggest Money Raising Vendors ')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10acdc",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "_**Make a histogram of the monthly distribution over 4 years of rides paid with cash**_\n",
    "\n",
    "First we will check if the data really has 4 years of data in it, then we do the histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14384f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''SELECT pickup_datetime\n",
    "FROM Trips\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "#Creating a list of years in the dataset, parsing the datetime format and getting the year of the datetime object\n",
    "datetimes = [datetime.strptime(c,'%Y-%m-%d %H:%M:%S').year for c in temp_df['pickup_datetime']]\n",
    "\n",
    "#Removing duplicates from list\n",
    "datetimes = list(set(datetimes))\n",
    "datetimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564fb5bd",
   "metadata": {},
   "source": [
    "So we do really have 4 years of data\n",
    "\n",
    "Now we need to check the unique values in the payment type column to see how we will get payments made in cash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db95c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''SELECT DISTINCT(payment_type)\n",
    "FROM Trips\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "temp_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "437fbd32",
   "metadata": {},
   "source": [
    "So _cash_ is written in full uppercase also, we need to handle that as we query the data we need.\n",
    "\n",
    "Now to make the histogram plot, for each month, of the total amount of rides paid with cash."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124dd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get the rows where payment was made with cash, and get the month that payment was made\n",
    "query='''SELECT payment_type, strftime('%m', pickup_datetime) as month\n",
    "FROM Trips\n",
    "WHERE lower(payment_type) like \"%cash%\"\n",
    "ORDER BY month asc\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "#Making the column into 1 values to be able to make the histogram work, can be interpreted as boolean for Cash \n",
    "temp_df['payment_type'] = 1\n",
    "\n",
    "fig = px.histogram(temp_df, x='month', y='payment_type', title='Trips paid with Cash Distribution by Month ')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912b693",
   "metadata": {},
   "source": [
    "We can see that cash payment drops **drastically** near the holidays. Probably because of the holidays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e23d83",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "_**Make a time series chart computing the number of tips each day for the last 3 months of\n",
    "2012**_\n",
    "\n",
    "Here we will need to get data ranging between the last three months of 2012, count the rows where tips were given (tip>0) and aggregate the counting by date. Then plot it as a time series.\n",
    "\n",
    "But it looks like the data only goes to the near end of october, so we won't get the three last months of the year exactly. Instead we will get the last 3 months of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231bfd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT SUM(case when tip_amount>0 then 1 else 0 end) as n_tips, strftime('%Y-%m-%d', pickup_datetime) as date\n",
    "FROM Trips\n",
    "GROUP BY date\n",
    "ORDER BY date desc\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e41e7",
   "metadata": {},
   "source": [
    "Here we will use a subquery to get the last date entry in our table, then use it to subtract 3 months to get the rows for the last 3 months of the year. \n",
    "\n",
    "We also need to sum the _tip_amount_ as 0 and 1 so we can count even the days where nobody gives tips, if there is such day. If a tip is given in a day (tip > 0), it is summed in the total, if no tip is given in a day, then we count 0 tips on that day. If we were to use _WHERE_ for this, we could end up excluding days with no tips from our query.\n",
    "\n",
    "Then we group by each day and plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1afdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query='''\n",
    "SELECT SUM(case when tip_amount>0 then 1 else 0 end) as n_tips, strftime('%Y-%m-%d', pickup_datetime) as date\n",
    "FROM Trips\n",
    "WHERE date BETWEEN DATE(   (SELECT strftime('%Y-%m-%d', pickup_datetime) as date\n",
    "                           FROM Trips\n",
    "                           ORDER BY date desc\n",
    "                           LIMIT 1),  \"-3 months\") \n",
    "                           \n",
    "                        and\n",
    "                        \n",
    "                        (SELECT strftime('%Y-%m-%d', pickup_datetime) as date\n",
    "                        FROM Trips\n",
    "                        ORDER BY date desc\n",
    "                        LIMIT 1) \n",
    "GROUP BY date\n",
    "ORDER BY date\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7176db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=temp_df['date']\n",
    "y=temp_df['n_tips']\n",
    "\n",
    "\n",
    "fig = go.Figure(data=go.Scatter(x=x, y=y, name='Number of tips p/day'))\n",
    "y=temp_df['n_tips'].rolling(5).mean()\n",
    "\n",
    "fig.add_trace(go.Scatter(x=x, y=y, name='M.A. 5 days'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c5f29b",
   "metadata": {},
   "source": [
    "# Bonus Tasks\n",
    "\n",
    "Here are the solutions to the bonus tasks of the test case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22146bf",
   "metadata": {},
   "source": [
    "## Bonus 1\n",
    "_**What is the average trip time on Saturdays and Sundays**_\n",
    "\n",
    "This is a query that needs alot of datetime manipulation. We need to get the difference between _pickup_datetime_ and _dropoff_datetime_. We also need to get the day of the week from this datetime. We will be showing the trip time in minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4897367",
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''SELECT AVG((julianday(dropoff_datetime) - julianday(pickup_datetime))* 24 * 60) as avg_trip_time,\n",
    "            cast (strftime('%w', pickup_datetime) as integer) as week_day\n",
    "FROM Trips\n",
    "WHERE week_day == 0 or week_day==6\n",
    "GROUP BY week_day\n",
    "'''\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)\n",
    "\n",
    "#Labeling the days of the week, that are returned as integers by the query\n",
    "temp_df.loc[temp_df['week_day']==0,'week_day'] = 'Sunday'\n",
    "temp_df.loc[temp_df['week_day']==6,'week_day'] = 'Saturday'\n",
    "\n",
    "#Getting the highest average trip time between both days\n",
    "avg = temp_df['avg_trip_time'].max()\n",
    "\n",
    "fig = px.bar(temp_df, x='week_day', y='avg_trip_time', title='AVG trip time in minutes by day')\n",
    "fig.add_hline(y=avg, line_dash = 'dash', line_color = 'firebrick',annotation_text=f\"Highest avg: {avg}\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ca6894",
   "metadata": {},
   "source": [
    "As we can see there is a minuscule difference between the average trip time in both days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f242fe59",
   "metadata": {},
   "source": [
    " ## Bonus 2\n",
    " \n",
    " _**Make a latitude and longitude map view of pickups and dropoffs in the year 2010**_\n",
    " \n",
    " \n",
    "This is a complicated task, since there are alot of data points and this can cause problems rendering our map. Luckly we have the _datashader_ module that can add datapoints to images as pixels, creating a fast and light plot. Perfect for big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c9e3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Query to get all the coordinates we need for the year 2010\n",
    "query='''SELECT pickup_latitude, pickup_longitude, dropoff_latitude, dropoff_longitude, strftime(\"%Y\",pickup_datetime) as year\n",
    "FROM Trips\n",
    "WHERE year = \"2010\"\n",
    "'''\n",
    "\n",
    "\n",
    "#Reading the query result into a dataframe\n",
    "temp_df = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fac32b",
   "metadata": {},
   "source": [
    "Now we will modify our dataframe so that we have all latitude and longitude coordinates, from pickups and dropoffs, in the same pair of columns. Then we will create a column which tells us if the coordinates are for a dropoff or a pickup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ed06bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datashader.utils import lnglat_to_meters as webm\n",
    "from functools import partial\n",
    "from datashader.utils import export_image\n",
    "from datashader.colors import colormap_select, Greys9\n",
    "from IPython.core.display import HTML, display\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Creating a new dataframe where we will store all coordinates\n",
    "newdf = pd.DataFrame()\n",
    "newdf['latitude'] = temp_df['pickup_latitude']\n",
    "newdf['longitude'] = temp_df['pickup_longitude']\n",
    "\n",
    "#Categorizing coordinates\n",
    "newdf['type'] = 'pickup'\n",
    "\n",
    "\n",
    "#Dataframe for all dropoffs\n",
    "drop_df=pd.DataFrame()\n",
    "drop_df['latitude'] = temp_df['dropoff_latitude']\n",
    "drop_df['longitude'] = temp_df['dropoff_longitude']\n",
    "drop_df['type'] = 'dropoff'\n",
    "\n",
    "#Changing the index, since we cant append Series objects with equal index\n",
    "drop_df.index = [c for c in range(newdf.shape[0], newdf.shape[0]+drop_df.shape[0])]\n",
    "\n",
    "#Appending the dropoff dataframe to the pickup dataframe\n",
    "newdf = newdf.append(drop_df)\n",
    "\n",
    "del drop_df\n",
    "\n",
    "#Setting a categorical column for our categories\n",
    "newdf['categorical'] = pd.Categorical(newdf['type'])\n",
    "\n",
    "#Filtering just in case\n",
    "types=['pickup','dropoff']\n",
    "newdf = newdf[newdf['type'].isin(types)]\n",
    "\n",
    "#Changing the type of coordinates to suit the needs of Datashader\n",
    "lon = newdf['longitude']\n",
    "lat = newdf['latitude']\n",
    "newdf.loc[:, 'easting'], newdf.loc[:, 'northing'] = webm(lon,lat)\n",
    "newdf.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301ddcc0",
   "metadata": {},
   "source": [
    "Here we will make the map view for the dropoffs and pickups. In **fuchsia** color pixels we have the dropoffs and in **aqua** pixels we have the pickups. We can see how dropoffs are very scattered around and pickups are very focused in the main parts of the city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59760b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#image resolution\n",
    "plot_width = int(1000)\n",
    "plot_height = int(1000)\n",
    "\n",
    "#Background color\n",
    "background = \"black\"\n",
    "\n",
    "\n",
    "export = partial(export_image, background = background, export_path=\"export\")\n",
    "cm = partial(colormap_select, reverse=(background!=\"black\"))\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "#Setting the borders of the map view. We have some outliers in our dataset that go far from NYC, which we don't want to affect the map view, so we use quantiles.\n",
    "y_range_min = lat.quantile(0.02)-0.10\n",
    "y_range_max = lat.quantile(0.98)+0.10\n",
    "x_range_min = lon.quantile(0.02)-0.10\n",
    "x_range_max = lon.quantile(0.98)+0.10\n",
    "\n",
    "#Passing the borders to the mapview\n",
    "sw = webm(x_range_min,y_range_min)\n",
    "ne = webm(x_range_max,y_range_max)\n",
    "SF = zip(sw, ne)\n",
    "\n",
    "#Color mapping for our categories, very important\n",
    "color_key = {'dropoff':'fuchsia' , 'pickup': 'aqua'}\n",
    "\n",
    "\n",
    "cvs = ds.Canvas(plot_width, plot_height, *SF)\n",
    "agg = cvs.points(newdf, 'easting', 'northing',agg=ds.count_cat('categorical'))\n",
    "\n",
    "img = export(tf.shade(agg, color_key =color_key, how='eq_hist'),\"sf_biz_grey\")\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521bd87",
   "metadata": {},
   "source": [
    "Now we would just need to overlay this on a real map from NYC and it would be perfect, but we still have some things to cover and this took some time to pull off. Maybe later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea5bce0",
   "metadata": {},
   "source": [
    "## Bonus 3\n",
    "_**Simulate JSON data streaming and view using a real-time metric with the Trips Data**_\n",
    "\n",
    "So we will need a class to simulate a streaming of data, where we can pull a continuous stream of JSON data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c74f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transaction:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Function to connect to local database and pull the data necessary for the data streaming task\n",
    "        \"\"\"\n",
    "        \n",
    "        query = '''SELECT vendor_id, total_amount as total_money_raised, dropoff_datetime\n",
    "                    FROM Trips\n",
    "                    ORDER BY dropoff_datetime\n",
    "                    \n",
    "                '''\n",
    "        \n",
    "        try: conn.cursor()\n",
    "        except: conn = sqlite3.connect(\"Trips_and_vendors.db\")\n",
    "        \n",
    "        #Reading the query result into a dataframe\n",
    "        self.df = pd.read_sql(query, conn)\n",
    "        self.row = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def serialize(self):\n",
    "        \"\"\"Function to mock retrieval of a row from a stream of data\n",
    "\n",
    "        :raises AttributeError: Error for when the Transaction class doesn't have attributes, meaning the connect function \n",
    "        was not used.\n",
    "        :return: A row of data from the class dataframe in JSON format, row by row as a stream.\n",
    "        :rtype: json dictionary\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            #restarts from row 0 if we reach end of dataframe\n",
    "            if self.row == self.df.shape[0]: self.row = 0 \n",
    "\n",
    "            #Gets a row slice of dataframe and transforms into json format\n",
    "            data = self.df.iloc[self.row].to_json()\n",
    "            self.row = self.row+1\n",
    "\n",
    "            return data\n",
    "        except AttributeError: raise AttributeError('No data to retrieve, please run the connect function first')\n",
    "        \n",
    "       \n",
    "    \n",
    "stream = Transaction()\n",
    "stream.connect()\n",
    "\n",
    "for _ in range(0,10):\n",
    "    print(stream.serialize())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cb94fa",
   "metadata": {},
   "source": [
    "And we will have to create a live plot of some metric to watch at real time. We will be watching the total amount of money collected by the taxi trips for each timestamp along the data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491236f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Deques are lists that have a maximum length, and as soon as they reach that length, they start popping out the first entries to it.\n",
    "#Perfect for our purposes\n",
    "X = deque(maxlen=20)\n",
    "Y = deque(maxlen=20)\n",
    "\n",
    "#Total money that will be summed up over time as the data stream comes\n",
    "total_money = 0\n",
    "\n",
    "#starting stream\n",
    "stream = Transaction()\n",
    "stream.connect()\n",
    "\n",
    "\n",
    "\n",
    "#Creating the Dash app for our live data plot\n",
    "app = dash.Dash(__name__)\n",
    "\n",
    "app.layout= html.Div([\n",
    "    \n",
    "    dcc.Graph(id='live-graph', animate=True),\n",
    "    \n",
    "    #interval of 1 second -> 1000 miliseconds\n",
    "    dcc.Interval(id='graph-update', interval=1000, n_intervals=0)\n",
    "    \n",
    "    \n",
    "])\n",
    "\n",
    "\n",
    "#Callback function that updates our graph in the given interval\n",
    "@app.callback(Output('live-graph','figure'),\n",
    "              [Input('graph-update', 'n_intervals')])\n",
    "def update_graph(*args):\n",
    "    \n",
    "    #Now, there probably is a better way to reference these variables, but just to make the plot work, let's \n",
    "    #leave it like this\n",
    "    global X\n",
    "    global Y\n",
    "    global total_money \n",
    "    \n",
    "    #Reading data from the stream\n",
    "    data = stream.serialize()\n",
    "    a_json = json.loads(data)\n",
    "    \n",
    "    #Summing up the money collected by taxi trips\n",
    "    total_money = total_money + a_json['total_money_raised']\n",
    "    \n",
    "    #Appending our data to the deques\n",
    "    Y.append(total_money)\n",
    "    X.append(a_json['dropoff_datetime'])\n",
    "    \n",
    "    #Line plot for our data\n",
    "    data=go.Scatter(x = list(X),\n",
    "                y=list(Y),\n",
    "                name='scatter',\n",
    "                mode='lines+markers'\n",
    "               )\n",
    "    \n",
    "    #Returns updated attributes for our figure/plot\n",
    "    return {'data':[data],\n",
    "            'layout': go.Layout(yaxis = dict(range=[min(Y), max(Y)]),\n",
    "                               xaxis = dict(range=[X[0], X[-1]]))}\n",
    "#Running Dash app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=False)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd2a550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
